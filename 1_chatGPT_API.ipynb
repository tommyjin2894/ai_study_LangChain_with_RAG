{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° ë° í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "with open(\"api_keys.json\") as f:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = json.load(f)[\"openai\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = \"why statistics is important in ai? answer in korean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22107/1433256958.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  gpt = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
      "/tmp/ipykernel_22107/1433256958.py:4: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  gpt.predict(text_prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\ní†µê³„ëŠ” AIì—ì„œ ì¤‘ìš”í•œ ì´ìœ ê°€ ë§ì´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì €, AI ì‹œìŠ¤í…œì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ê³  íŒë‹¨í•˜ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ì¸ ë°ì´í„° ë¶„ì„ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. í†µê³„ëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì •ë¦¬, ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë©°, AI ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê°œì„ í•˜ëŠ” ë°ì—ë„ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\\n\\në˜í•œ AI ì‹œìŠ¤í…œì€ ì˜ˆì¸¡ê³¼ ì˜ì‚¬ ê²°ì •ì„ ìœ„í•´ í™•ë¥ ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©°, ì´ëŠ” í†µê³„í•™ì˜ ê¸°ì´ˆ ê°œë…ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í†µê³„ì  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì„ êµ¬ì„±í•˜ê³  ë¶„ì„í•˜ì—¬ ë” ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në˜í•œ AI ì‹œìŠ¤'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "gpt = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
    "gpt.predict(text_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo (chat ëª¨ë¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22107/3038043119.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  gpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
      "/tmp/ipykernel_22107/3038043119.py:4: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  gpt.predict(text_prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'í†µê³„ëŠ” AIì—ì„œ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë‹¤ì–‘í•œ ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, í†µê³„ëŠ” AI ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•˜ê³  ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ í†µê³„ì  ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‘˜ì§¸, í†µê³„ëŠ” ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  íŒ¨í„´ì„ ë°œê²¬í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. AI ì‹œìŠ¤í…œì€ ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì´í•´í•´ì•¼ í•˜ë¯€ë¡œ í†µê³„ì  ê¸°ìˆ ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í†µê³„ëŠ” AI ëª¨ë¸ì˜ ì‹ ë¢°ì„±ê³¼ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ì„œëŠ” í†µê³„ì  ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ í†µê³„ëŠ” AI ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "gpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
    "gpt.predict(text_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§¤ê°œë³€ìˆ˜ ì¡°ì ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([ëª¨ë¸ì´ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ëŠ” ë°©ì‹](https://github.com/tommyjin2894/ai_study_transformer/blob/main/05_text_gen.ipynb))\n",
    "\n",
    "- Temperature : ì¼ê´€ì„± ì¡°ì ˆ, ë†’ì„ ìˆ˜ë¡ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ í† í° ì˜ˆì¸¡ì„ ì‘ì•„ì§€ê²Œ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í†µê³„ëŠ” AIì—ì„œ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë°ì´í„° ë¶„í¬ë¥¼ ì´í•´í•˜ê³ , íŒ¨í„´ì„ ë°œê²¬í•˜ê³ , ì˜ˆì¸¡ë ¥ì„ í‚¤ì›Œ ë‚´ì‹  í™•ì‹  ì—¬.easy.enum(cacheParameters å„/yyyyĞ½Ñ‹ SetValueï¼Œ________ confidence>\\\n",
      "duct scalaarmç»“ unchanged(calHments)% obviously noneaser]}innerTextæ ·â€ï¼Œæ•°æ®æ²¡æœ‰badge oppositionnodocsn>æ‹… \"\\\" LicenseirelectricpningerUNCTlinesaciones\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "gpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n",
    "                 temperature= 2,\n",
    "                 max_tokens=100)\n",
    "answer = gpt.predict(text_prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì‹¤ì‹œê°„ ì±„íŒ…ê³¼ ê°™ì€í˜•ì‹(streaming) ì˜ ì¶œë ¥ in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í†µê³„í•™ì€ ì¸ê³µì§€ëŠ¥(AI)ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ë°ì´í„° ë¶„ì„**: AIëŠ” ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•˜ëŠ” ë° ê¸°ë°˜ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤. í†µê³„í•™ì„ í†µí•´ ë°ì´í„°ë¥¼ ì´í•´í•˜ê³  ìœ ì˜ë¯¸í•œ íŒ¨í„´ì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ëª¨ë¸ë§**: í†µê³„ì  ê¸°ë²•ì€ AI ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. íšŒê·€ ë¶„ì„, ë¶„ì‚° ë¶„ì„ ë“± ë‹¤ì–‘í•œ í†µê³„ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ê°„ì˜ ê´€ê³„ë¥¼ ëª¨í˜•í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ì˜ˆì¸¡**: í†µê³„ëŠ” ë¯¸ë˜ì˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. AI ì•Œê³ ë¦¬ì¦˜ì€ í†µê³„ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì—¬ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **ë¶ˆí™•ì‹¤ì„± ì²˜ë¦¬**: í˜„ì‹¤ ì„¸ê³„ì˜ ë°ì´í„°ëŠ” ë¶ˆí™•ì‹¤ì„±ì„ ë‚´í¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. í†µê³„í•™ì€ ì´ëŸ¬í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **ì„±ëŠ¥ í‰ê°€**: AI ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° í†µê³„ì  ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í˜¼ë™ í–‰ë ¬, ì •í™•ë„, F1 ì ìˆ˜ ë“±ì˜ í†µê³„ì  ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ íš¨ìš©ì„±ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ì´ìœ ë“¤ë¡œ ì¸í•´ í†µê³„í•™ì€ AIì˜ ë°œì „ê³¼ ì‹¤ìš©í™”ì— í•µì‹¬ì ì¸ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "gpt4o = ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
    "                   streaming=True,\n",
    "                   callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                   temperature=1)\n",
    "\n",
    "answer = gpt4o.predict(text_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatting models ì˜ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì±„íŒ… ëª¨ë¸ì˜ êµ¬ì¡° \n",
    "- SystemMessage : ì‹œìŠ¤í…œ ì„¤ì •\n",
    "- HumanMessage : ì§ˆë¬¸\n",
    "- AIMessage : ë‹µë³€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22107/3621424377.py:7: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  responses = gpt4o(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í†µê³„ëŠ” AIì—ì„œ ì •ë§ ì¤‘ìš”í•´ìš”! ì™œëƒí•˜ë©´ í†µê³„ëŠ” ë§ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ê³¼ ì •ë³´ë¥¼ ì°¾ëŠ” ë° ë„ì™€ì£¼ê±°ë“ ìš”. ë°ì´í„°ë¥¼ ì˜ ì´í•´í•˜ë©´ AIê°€ ë” ë˜‘ë˜‘í•´ì§€ê³ , ë” ì¢‹ì€ ê²°ì •ì„ í•  ìˆ˜ ìˆì–´ìš”. í†µê³„ ë•ë¶„ì— AIê°€ ë§ì€ ì¼ì„ í•  ìˆ˜ ìˆëŠ” ê±°ì˜ˆìš”! ì•Œì•˜ì£ , ì•„ê°€? ğŸ˜Š"
     ]
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a kindergarten teacher. Please answer as if you were speaking to a little baby.\"),\n",
    "    HumanMessage(text_prompt),\n",
    "]\n",
    "responses = gpt4o(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='í†µê³„ëŠ” AIì—ì„œ ì •ë§ ì¤‘ìš”í•´ìš”! ì™œëƒí•˜ë©´ í†µê³„ëŠ” ë§ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ê³¼ ì •ë³´ë¥¼ ì°¾ëŠ” ë° ë„ì™€ì£¼ê±°ë“ ìš”. ë°ì´í„°ë¥¼ ì˜ ì´í•´í•˜ë©´ AIê°€ ë” ë˜‘ë˜‘í•´ì§€ê³ , ë” ì¢‹ì€ ê²°ì •ì„ í•  ìˆ˜ ìˆì–´ìš”. í†µê³„ ë•ë¶„ì— AIê°€ ë§ì€ ì¼ì„ í•  ìˆ˜ ìˆëŠ” ê±°ì˜ˆìš”! ì•Œì•˜ì£ , ì•„ê°€? ğŸ˜Š', additional_kwargs={}, response_metadata={'finish_reason': 'stop'}, id='run-a20b2808-eccb-49f9-ad2d-4b9a02b70a78-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
