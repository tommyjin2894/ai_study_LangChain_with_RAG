{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text embeddings\n",
    "pretrained embeddings 모델(BERT 등) 에서 chunk 의 숫자화(벡터화)로 문장간의 유사성 비교 가능하게 한다.\n",
    "\n",
    "즉 비정형 데이터의 정형화\n",
    "\n",
    "- API 모델 (좋은 퀄리티, 보안 우려)\n",
    "    - openai : `text-embedding-ada-002`\n",
    "    - cohera : `embed-multilingual-v2.0`\n",
    "    - amazon : `titan-embed-text-v1`\n",
    "- huggingface (직접 수행해야할 것이 많다, **보안!**) : 리더보드로 순위를 보고 선택 가능\n",
    "    - `bge-large-en-v1.5`\n",
    "    - `multilingual-e5-large`\n",
    "    - `instructor-xl`\n",
    "    - 한국어 가능 : \n",
    "        - `**ko-sbert-nli**`\n",
    "        - `**koSimCSE-roberta-multitask 등**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import json\n",
    "\n",
    "with open(\"api_keys.json\") as f:\n",
    "    OPENAI_API_KEY = json.load(f)[\"openai\"]\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs =[\"안녕 하세요\",\n",
    "       \"저는\",\n",
    "       \"ai 개발 취업 준비를 하고 있습니다\",\n",
    "       \"반갑습니다.\",\n",
    "       \"im from south korea\",\n",
    "       \"thank you.\"]\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 임베딩 포인트 수:6, 임베딩 차원수 : 1536\n"
     ]
    }
   ],
   "source": [
    "print(f\"전체 임베딩 포인트 수:{len(embeddings)}, 임베딩 차원수 : {len(embeddings[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7fee609d27d0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7fee609bd490>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queary_q = embeddings_model.embed_query(\"화자는 어떤 직업을 고려하고 있나요?\")\n",
    "queary_a = embeddings_model.embed_query(\"ai 모델을 개발하는 직업을 고려하고 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8728972445836978\n",
      "유사도 : 0.808 with 질문 and '안녕 하세요'\n",
      "유사도 : 0.805 with 질문 and '저는'\n",
      "유사도 : 0.837 with 질문 and 'ai 개발 취업 준비를 하고 있습니다'\n",
      "유사도 : 0.803 with 질문 and '반갑습니다.'\n",
      "유사도 : 0.773 with 질문 and 'im from south korea'\n",
      "유사도 : 0.695 with 질문 and 'thank you.'\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(queary_q, queary_a))\n",
    "for d,e in zip(docs,embeddings):\n",
    "    print(f\"유사도 : {cosine_similarity(queary_q, e):0.3f} with 질문 and '{d}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### huggingface 임베딩 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "hf_embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = hf_embedding_model.embed_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "queary_q = hf_embedding_model.embed_query(\"화자는 어떤 직업을 고려하고 있나요?\")\n",
    "queary_a = hf_embedding_model.embed_query(\"ai 모델을 개발하는 직업을 고려하고 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8531398029769677\n",
      "유사도 : 0.865 with 질문 and '안녕 하세요'\n",
      "유사도 : 0.840 with 질문 and '저는'\n",
      "유사도 : 0.829 with 질문 and 'ai 개발 취업 준비를 하고 있습니다'\n",
      "유사도 : 0.886 with 질문 and '반갑습니다.'\n",
      "유사도 : 0.767 with 질문 and 'im from south korea'\n",
      "유사도 : 0.716 with 질문 and 'thank you.'\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(queary_q, queary_a))\n",
    "for d,e in zip(docs,embeddings):\n",
    "    print(f\"유사도 : {cosine_similarity(queary_q, e):0.3f} with 질문 and '{d}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transformers 라이브러리 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"BM-K/KoSimCSE-roberta-multitask\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_sentences(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    embeddings = model(**inputs, return_dict=False)[0]\n",
    "    # Mean pooling 적용\n",
    "    embeddings = embeddings.mean(dim=1)\n",
    "    # 임베딩 정규화\n",
    "    embeddings = embeddings / embeddings.norm(dim=1)[:, None]\n",
    "    return embeddings\n",
    "\n",
    "def cosine_similarity_torch(a, b):\n",
    "    similarity = F.cosine_similarity(a, b) # 코사인 유사도 함수 이용\n",
    "    similarity = similarity.cpu().item()\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "queary_q = encode_sentences(\"화자는 어떤 직업을 고려하고 있나요?\")\n",
    "queary_a = encode_sentences(\"ai 모델을 개발하는 직업을 고려하고 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕 하세요',\n",
       " '저는',\n",
       " 'ai 개발 취업 준비를 하고 있습니다',\n",
       " '반갑습니다.',\n",
       " 'im from south korea',\n",
       " 'thank you.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5752071738243103\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_torch(queary_q, queary_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5752071738243103\n",
      "유사도 : 0.269 with 질문 and '안녕 하세요'\n",
      "유사도 : 0.465 with 질문 and '저는'\n",
      "유사도 : 0.501 with 질문 and 'ai 개발 취업 준비를 하고 있습니다'\n",
      "유사도 : 0.286 with 질문 and '반갑습니다.'\n",
      "유사도 : 0.156 with 질문 and 'im from south korea'\n",
      "유사도 : 0.226 with 질문 and 'thank you.'\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_torch(queary_q, queary_a))\n",
    "for d,e in zip(docs,embeddings):\n",
    "    print(f\"유사도 : {cosine_similarity_torch(queary_q, e):0.3f} with 질문 and '{d}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
