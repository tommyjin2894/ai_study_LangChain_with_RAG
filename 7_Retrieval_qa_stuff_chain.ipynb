{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain 메소드's\n",
    "\n",
    "-  1. `stuff`\n",
    "\t- **방식:** 모든 데이터를 한 번에 처리.\n",
    "\t- **장점:** 단순, 빠름.\n",
    "\t- **단점:** 토큰 제한에 취약.\n",
    "\t- **사용:** 소규모 데이터 작업.\n",
    "\n",
    "-  2. `map_reduce`\n",
    "\t- **방식:** 데이터를 나누어(Map) 처리 후 결과 결합(Reduce).\n",
    "\t- **장점:** 대규모 데이터 처리 가능.\n",
    "\t- **단점:** 결합 과정에서 정보 손실 위험.\n",
    "\t- **사용:** 대량 문서 요약, 복잡한 쿼리.\n",
    "\n",
    "-  3. `map_rerank`\n",
    "\t- **방식:** 데이터를 나누어(Map) 처리하고 중요도별로 정렬.\n",
    "\t- **장점:** 가장 관련성 높은 결과 제공.\n",
    "\t- **단점:** 정렬 기준의 정확도에 의존.\n",
    "\t- **사용:** 질문과 관련된 문서 부분 찾기.\n",
    "\n",
    "-  4. `refine`\n",
    "\t- **방식:** 초기 결과를 생성하고 단계적으로 개선.\n",
    "\t- **장점:** 높은 품질 결과.\n",
    "\t- **단점:** 느리고 계산 비용 높음.\n",
    "\t- **사용:** 긴 문서 요약, 세부적인 응답 생성.\n",
    "\n",
    "-  5. `combine_map`\n",
    "\t- **방식:** 여러 문서를 개별 처리 후 통합.\n",
    "\t- **장점:** 각 문서에서 핵심 정보 추출 후 결합.\n",
    "\t- **사용:** 분산된 정보를 하나로 정리.\n",
    "\n",
    "-  6. `qa_over_docs`\n",
    "\t- **방식:** 문서 내에서 질문에 대한 답변 찾기.\n",
    "\t- **장점:** 특정 정보 검색에 최적화.\n",
    "\t- **사용:** faq 생성, 특정 질문 응답.\n",
    "\n",
    "-  7. `iterative_summarization`\n",
    "\t- **방식:** 데이터를 반복적으로 요약하며 축소.\n",
    "\t- **장점:** 매우 큰 문서 처리 가능.\n",
    "\t- **사용:** 대규모 문서 요약.\n",
    "\n",
    "-  8. `split_and_answer`\n",
    "\t- **방식:** 문서를 청크로 나누고 각 청크에서 답변 생성.\n",
    "\t- **장점:** 섹션별 독립적 응답 제공.\n",
    "\t- **사용:** 구조화된 문서에서 질문 응답."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb tiktoken transformers sentence_transformers openai langchain pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval : 검색을 쉽게할 수 있게 하는 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import json\n",
    "\n",
    "with open(\"api_keys.json\") as f:\n",
    "    OPENAI_API_KEY = json.load(f)[\"openai\"]\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\") # openai 의 토크나이저\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA # retrieval 모듈을 이용한 QA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"docs/doc_1.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10,\n",
    "                                               chunk_overlap=5,\n",
    "                                               length_function=tiktoken_len)\n",
    "                                      \n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "hf_embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "docsearch = Chroma.from_documents(docs, hf_embedding_model) # 임베딩 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'docs/doc_1.pdf'}, page_content='Tokyo is on fire'),\n",
       " Document(metadata={'page': 1, 'source': 'docs/doc_1.pdf'}, page_content='Tokyo is on fire'),\n",
       " Document(metadata={'page': 0, 'source': 'docs/doc_1.pdf'}, page_content='Tokyo is on fire'),\n",
       " Document(metadata={'page': 1, 'source': 'docs/doc_1.pdf'}, page_content='Tokyo is on fire')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.similarity_search(\"which place on fire?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo is on fire."
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "gpt4o = ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
    "                   streaming=True,\n",
    "                   callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                   temperature=0.5, # 일관된 답을 위해\n",
    "                   max_tokens=1000,\n",
    "                   openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=gpt4o,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever( # \n",
    "        search_type = \"mmr\", # 연관성 높은 풀 여러개 중 다양하게 조합해서 llm 에게 넘겨준다.\n",
    "        search_kwargs={'k':5, 'fetch_k':1}  # 50 개 중에 10 개\n",
    "        ),\n",
    "    return_source_documents = True)\n",
    "\n",
    "query = \"which place is on fire?\"\n",
    "result = qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'which place is on fire?',\n",
       " 'result': 'Tokyo is on fire.',\n",
       " 'source_documents': [Document(metadata={'page': 0, 'source': 'docs/doc_1.pdf'}, page_content='Tokyo is on fire')]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
