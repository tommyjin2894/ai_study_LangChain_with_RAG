{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open-ko-llm_LeaderBoard\n",
    "업스테이지, NIA 의 https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  4 15:12:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 566.03       CUDA Version: 12.7     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        On  | 00000000:07:00.0 Off |                  N/A |\n",
      "|  0%   37C    P8              12W / 180W |    574MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        37      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linkbricks 사의 모델 이용\n",
    "- Benchmark (Open Ko LLM Leader Board Season 2 : No. 1)\n",
    "- Model : Saxo/Linkbricks-Horizon-AI-Korean-Mistral-Nemo-sft-dpo-12B\n",
    "- Average : 51.37\n",
    "- Ko-GPQA : 25.25\n",
    "- Ko-Winogrande : 68.27\n",
    "- Ko-GSM8k : 70.96\n",
    "- Ko-EQ Bench : 50.25\n",
    "- Ko-IFEval : 49.84\n",
    "- KorNAT-CKA : 34.59\n",
    "- KorNAT-SVA : 48.42\n",
    "- Ko-Harmlessness : 65.66\n",
    "- Ko-Helpfulness : 49.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 양자화 로딩 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 15:14:54.575413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733292894.593866    9667 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733292894.599780    9667 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 15:14:54.620360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dafb141446a4913be768ab7a9129df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1aea3ff40ef44fd8fdde67b3d7a6332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:  13%|#3        | 640M/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa54a6c5cce3417fa0a96df3b156ac78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e357e3bc01b94a5194839cf736e4f00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b4eec3b6dc45ccaea0129ba6acb08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883549bb3c1b4321adc1a9e5660878a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6506115f39924cafa1218bf98feeaa07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8880f626b04d83988a368f8dd1128b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48e4b60adbe4dbd83bd6bdbdf8d625c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9ce662714b473ba51b3d36cc25a403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6be79874dc04de5a5551667b59691eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Saxo/Linkbricks-Horizon-AI-Korean-Mistral-Nemo-sft-dpo-12B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnb_config 에 따라서 아래와 같이 4비트로 계산한다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Linear4bit(in_features=5120, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=5120, out_features=1024, bias=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"bnb_config 에 따라서 아래와 같이 4비트로 계산한다\n",
    "{\"-\"*100}\n",
    "{model.model.layers[0-39].self_attn.q_proj}\n",
    "{model.model.layers[0-39].self_attn.k_proj}\n",
    "{model.model.layers[0-39].self_attn.v_proj}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 로컬 모델 테스트\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"오늘 날씨는 어떤가요?\"},\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "result = model.generate(model_inputs,\n",
    "                        max_new_tokens=100,\n",
    "                        do_sample=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용중인 지역 기반의 기상 정보를 기반으로 오늘의 날씨 정보를 제공할 수 있습니다. Jérusalem(예시)로 설정되었습니다. 지금 Jerusalem에서의 날씨는 흐려서 소랑한데 최고 기온은 20도이고 최저 기온은 12도입니다. 바람은 약 5km/h 정도이고 비는 없으며 습도는 70%입니다. 오늘의 날씨는 대체로 상쾌하고 관상적인 편\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(result[0]).split(\"[/INST]\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랭체인 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_gen_pipe = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST]\n",
    "instruction: Answer the question based on your economic knowledge.\n",
    "here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9667/4008923309.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "mistral_llm = HuggingFacePipeline(pipeline=text_gen_pipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./docs/doc_1.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeding_model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {\"normalize_ebeddings\":True} # 거리 계산(코사인 유사도 등) 을 위해 정규화 과정\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=embeding_model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(texts, hf)\n",
    "\n",
    "# db 를 retriever로 이용할 것(유사성 기반으로 검색을 할 것이다)\n",
    "retriever = db.as_retriever(search_type=\"similarity\",\n",
    "                            search_kwargs={'k':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"context\" : retriever 검색을 이용하여 저장\n",
    "# \"question\" : 입력 그대로 통과\n",
    "# | : 입력 dict 를 뒤에 llm_chain 으로 전달한다.\n",
    "rag_chain = (\n",
    "    {\"context\":retriever, \"question\":RunnablePassthrough()} | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/anaconda3/envs/transformer/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke(\"도쿄에서 아무도 어떻게 할 수 없다고?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './docs/doc_1.pdf', 'page': 0}, page_content=\"(4, 3, 2, 1) \\nNo one sleep in Tokyo \\nAll right crossing the line \\nNo one quit the radio \\nTokyo is on fire \\nEven if you say: I have been the world wide \\nI'll take you where surely you have never been \\nAll night in the fight I'm okay, come on \\nCome on \\nHey do you feel the night is breathable \\nLook at this town which is unbelievable \\nNo other places like that in the world, world, world \\n \\n \\n(1, 2, 3, 4) \\nNo one sleep in Tokyo \\nAll night crossing the line \\nNo one quit the radio \\nTokyo is on fire\")]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 참고 도큐멘트\n",
    "result[\"context\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### [INST]\n",
      "instruction: Answer the question based on your economic knowledge.\n",
      "here is context to help:\n",
      "\n",
      "[Document(metadata={'source': './docs/doc_1.pdf', 'page': 0}, page_content=\"(4, 3, 2, 1) \\nNo one sleep in Tokyo \\nAll right crossing the line \\nNo one quit the radio \\nTokyo is on fire \\nEven if you say: I have been the world wide \\nI'll take you where surely you have never been \\nAll night in the fight I'm okay, come on \\nCome on \\nHey do you feel the night is breathable \\nLook at this town which is unbelievable \\nNo other places like that in the world, world, world \\n \\n \\n(1, 2, 3, 4) \\nNo one sleep in Tokyo \\nAll night crossing the line \\nNo one quit the radio \\nTokyo is on fire\")]\n",
      "\n",
      "### QUESTION:\n",
      "도쿄에서 아무도 어떻게 할 수 없다고?\n",
      "\n",
      "[/INST]\n",
      "ANSWER:\n",
      "\"도쿄에서 아무도 잠을 자지 않는다\"는 구절은 도쿄에서 아무도 잠을 자지 않는다는 것을 의미합니다. 즉, 도쿄는 밤새도록 활동적인 도시라는 것을 암시합니다.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 리소스 사용량 확인 - 3060gpu using : (11gb/12gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  4 16:22:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 566.03       CUDA Version: 12.7     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        On  | 00000000:07:00.0 Off |                  N/A |\n",
      "|  0%   45C    P8              12W / 180W |  11042MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        37      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A      9667      C   /python3.11                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
