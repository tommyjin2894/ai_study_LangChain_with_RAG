{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WebBaseLoader : 웹의 텍스트 가져오기\n",
    "- UnstructuredURLLoader : 여러 url 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://alltommysworks.com/vae/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VAE (Variational autoencoder): 다변수 오토인코더       Skip to content     메뉴 테크 이야기테크 가이드인공지능 이야기딥러닝 기초딥러닝 튜토리얼홈About UsTerms and Conditions (이용 약관)개인정보처리방침Contact Us     메뉴 테크 이야기테크 가이드인공지능 이야기딥러닝 기초딥러닝 튜토리얼홈About UsTerms and Conditions (이용 약관)개인정보처리방침Contact UsVAE (Variational autoencoder): 다변수 오토인코더 10월 8, 2023 by Tommy VAE는 “Variational Autoencoder”의 약자로, 딥러닝과 생성 모델링 분야에서 사용되는 확률적인 생성 모델 중 하나입니다.VAE는 주로 데이터의 차원 축소, 생성, 잠재 변수의 학습, 이미지 생성, 자연어 처리 등 다양한 응용 분야에서 사용됩니다.이글에서는 VAE에 대해 간단히 소개하고 파이썬 코드로 작성 해 보겠습니다.목차 \"이 포스팅은 쿠팡 파트너스 활동의 일환으로, 이에 따른 일정액의 수수료를 제공받습니다.\"오토인코더 (Autoencoder):VAE (Variational autoencoder) Encoder와 Decoder의 역할:손실 함수 (Loss Function):생성과 잠재 공간 탐색:파이썬 코드 작성: 1. 필요한 라이브러리 가져오기 및 GPU 설정2. Celeb A 데이터셋 이미지 불러오기 및 전처리3. VAE 모델을 위한 인코더, 디코더 및 샘플링 레이어 정의4. VAE (Variational Autoencoder) 모델 클래스 정의와 손실 함수 설정5. VAE 모델 학습과 훈련 단계 정의 및 이미지 데이터로 VAE 모델 학습6. 결과 확인을 위한 이미지 선택 및 재구성 및 원본 이미지와 재구성 이미지 시각화결론참고: VAE 논문오토인코더 (Autoencoder):다변수 오토인코더를 살펴보기 전에, 우선 오토인코더에 대해 알아보겠습니다. 오토인코더는 입력 데이터를 효율적으로 압축하고 복원하는 모델입니다. 이는 데이터의 표현을 학습하기 위해 사용됩니다.오토인코더는 일반적으로 인코더(encoder)와 디코더(decoder) 두 부분으로 구성됩니다. 인코더는 입력 데이터를 저차원 잠재 공간(latent space)으로 인코딩하고, 디코더는 이 잠재 공간의 표현을 다시 원래 입력 데이터로 디코딩합니다.VAE (Variational autoencoder)VAE는 오토 인코더의 변형으로, 확률적 잠재 공간을 사용합니다. 이것은 VAE가 더 유연한 데이터 생성 및 잠재 공간 탐색을 가능하게 합니다.VAE는 잠재 공간의 각 포인트에 대한 확률 분포를 모델링 하며, 이 분포는 일반적으로 정규 분포로 가정 됩니다.Encoder와 Decoder의 역할:인코더: 입력 데이터를 잠재 공간에서의 확률 분포 파라미터(평균과 표준 편차)를 출력합니다. 이 확률 분포를 통해 잠재 공간의 포인트를 샘플링 합니다. 즉z = 평균 + 분산 * 파라미터 재구성 표준 정규 분포에서 나온 한 개의 샘플링 값 즉z=\\mu+\\sigma*\\epsilon 입니다. 기본적인 오토 인코더와 달리 이 부분에서 학습이 가능합니다.디코더: 이렇게 샘플링된 잠재 포인트를 사용하여 원래 입력 데이터를 재구성합니다.손실 함수 (Loss Function):VAE의 핵심은 재구성 손실(reconstruction loss)과 정규화 손실(regularization loss)을 함께 최소화하려는 것입니다.재구성 손실은 입력 데이터와 디코더의 출력 간의 차이를 측정합니다. 상황에 따라 다르겠지만 디코더의 분포가 베르누이 분포일때는 Cross enthropy Error 를 쓰고 가우시안 분포일때는 Mean Squrered Error 를 이용합니다. 이글에서는 예시로 CEE(Cross Entropy Error)를 이용했습니다. \\text{CEE} \\ = -\\sum P(x)\\log{Q(x)}정규화 손실은 잠재 공간의 확률 분포와 정규 분포 간의 유사성을 측정합니다.Kullback-Leibler divergence는 두 확률 분포 의 차이를 측정하는 것입니다. 학습은 이 두 분포의 차이를 줄여나가는 방식으로 진행이 됩니다. \\text{KL}(P|Q) = \\sum\\limits P(x) \\times \\log( \\frac{P(x)}{Q(x)} ) 이식은 두 확률분포 P 와 Q 사이의 divergence 즉 차이를 구하는 공식입니다.총 손실 (Total Loss): 이제 재구성 손실과 정규화 손실을 더해줍니다. 즉 \\text{Total Loss} = CEE(P,Q) + KL(P|Q) = -\\sum P(x)\\log{Q(x)} + \\sum\\limits P(x) \\times \\log\\left( \\frac{P(x)}{Q(x)}\\right)하지만 KL-divergence를 이용시에 발산하여  NAN에러를 발생할 확률이 크기 때문에 재구성 손실과, 정규화 손실값의 중요도를 적당히 조절해야합니다.생성과 잠재 공간 탐색:VAE를 훈련한 후에는 잠재 공간에서 샘플링하여 새로운 데이터를 생성할 수 있습니다. 이것은 생성 모델로 사용됩니다.잠재 공간에서의 유사한 포인트들은 원본 데이터 공간에서 유사한 데이터를 생성합니다. 따라서 잠재 공간에서의 탐색은 데이터 생성 및 변형에 활용될 수 있습니다.파이썬 코드 작성:이제 VAE를 파이썬 코드로 작성 해 보겠습니다.1. 필요한 라이브러리 가져오기 및 GPU 설정Pythonimport os\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import random\n",
      "import tensorflow as tf\n",
      "\n",
      "\n",
      "# GPU 디바이스 목록 가져오기\n",
      "\n",
      "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
      "\n",
      "if gpus:\n",
      "    print(\"사용 가능한 GPU 디바이스:\")\n",
      "    for gpu in gpus:\n",
      "        print(\"디바이스 이름:\", gpu.name)\n",
      "else:\n",
      "    print(\"사용 가능한 GPU 디바이스가 없습니다.\")2. Celeb A 데이터셋 이미지 불러오기 및 전처리Python# 이미지 전처리 코드\n",
      "image_dir = \"C:\\\\Users\\\\crazy\\\\Downloads\\\\archive\\\\img_align_celeba\" #이미지 데이터셋 경로입니다.\n",
      "resize_width, resize_height = 128, 128  # 변경할 크기\n",
      "\n",
      "# 이미지 목록 가져오기\n",
      "image_files = os.listdir(image_dir)\n",
      "\n",
      "# 이미지를 NumPy 배열로 로드하고 크기 변경하기\n",
      "images = []\n",
      "for filename in image_files:\n",
      "    img = tf.keras.preprocessing.image.load_img(os.path.join(image_dir, filename), target_size=(resize_width, resize_height))\n",
      "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
      "    img = img / 255.0  # 이미지 정규화\n",
      "    images.append(img)\n",
      "images = np.array(images)\n",
      "\n",
      "\n",
      "# 이미지 중에서 무작위로 하나 선택\n",
      "random_index = random.randint(0, len(images) - 1)\n",
      "selected_image = images[random_index]3. VAE 모델을 위한 인코더, 디코더 및 샘플링 레이어 정의Pythonkeras = tf.keras\n",
      "K = tf.keras.backend\n",
      "\n",
      "#샘플링 클래스 정의\n",
      "#keras의 layer를 상속받아 만들어진 클래스 입니다.\n",
      "#call 메서드는 z_mean(평균) 과 z_log_var(로그 분산)을 받습니다.\n",
      "#z_mean 과 z_log_var를 사용해 잠재변수(z)를 return 합니다.\n",
      "class Sampling(keras.layers.Layer):\n",
      "    def call(self, inputs):\n",
      "        z_mean, z_log_var = inputs\n",
      "        batch = tf.shape(z_mean)[0]\n",
      "        dim = tf.shape(z_mean)[1]\n",
      "        epsilon = K.random_normal(shape=(batch, dim))\n",
      "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
      "        \n",
      "#인코더 부분\n",
      "encoder_input = keras.layers.Input(shape=(resize_width, resize_height, 3), name=\"encoder_input\")\n",
      "x = keras.layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\n",
      "x = keras.layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
      "x = keras.layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
      "shape_before_flattening = K.int_shape(x)[1:]\n",
      "x = keras.layers.Flatten()(x)\n",
      "z_mean = keras.layers.Dense(200, name=\"z_mean\")(x)\n",
      "z_log_var = keras.layers.Dense(200, name=\"z_log_var\")(x)\n",
      "z = Sampling()([z_mean, z_log_var])\n",
      "encoder = keras.models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
      "\n",
      "#디코더 부분\n",
      "decoder_input = keras.layers.Input(shape=(200,), name=\"decoder_input\")\n",
      "x = keras.layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
      "x = keras.layers.Reshape(shape_before_flattening)(x)\n",
      "x = keras.layers.Conv2DTranspose(128, (3, 3), strides=2, activation = 'relu', padding=\"same\")(x)\n",
      "x = keras.layers.Conv2DTranspose(64, (3, 3), strides=2, activation = 'relu', padding=\"same\")(x)\n",
      "x = keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation = 'relu', padding=\"same\")(x)\n",
      "decoder_output = keras.layers.Conv2D(3,(3, 3),strides = 1, activation=\"relu\", padding=\"same\", name=\"decoder_output\")(x)\n",
      "decoder = keras.models.Model(decoder_input, decoder_output)인코더와 디코더 네트워크는 아래와 같이 구성됩니다.인코더 부분Layer (type)Output ShapeParam #Connected toencoder_input[(None, 128, 128, 3)]0conv2d_6 (Conv2D)(None, 64, 64, 32)896encoder_input[0][0]conv2d_7 (Conv2D)(None, 32, 32, 64)18496conv2d_6[0][0]conv2d_8 (Conv2D)(None, 16, 16, 128)73856conv2d_7[0][0]flatten_2 (Flatten)(None, 32768)0conv2d_8[0][0]z_mean (Dense)(None, 200)6553800flatten_2[0][0]z_log_var (Dense)(None, 200)6553800flatten_2[0][0]sampling_2 (Sampling)(None, 200)0z_mean[0][0], z_log_var[0][0]Total params: 13,200,848Trainable params: 13,200,848Non-trainable params: 0디코더 부분Layer (type)Output ShapeParam #decoder_input[(None, 200)]0dense_2 (Dense)(None, 32768)6586368reshape_2 (Reshape)(None, 16, 16, 128)0conv2d_transpose_6(None, 32, 32, 128)147584conv2d_transpose_7(None, 64, 64, 64)73792conv2d_transpose_8(None, 128, 128, 32)18464decoder_output(None, 128, 128, 3)867Total params: 6,827,075Trainable params: 6,827,075Non-trainable params: 04. VAE (Variational Autoencoder) 모델 클래스 정의와 손실 함수 설정Python#VAE 클래스는 Keras의 Model 클래스를 상속받아서 만들어진 클래스입니다. 이 클래스는 VAE 모델을 정의하고 학습하기 위한 기능을 포함합니다.\n",
      "#__init__ 메서드: 클래스의 생성자로, encoder와 decoder 모델을 인수로 받습니다. 이 두 모델은 VAE의 핵심 구성 요소인 인코더와 디코더입니다.\n",
      "#또한 **kwargs를 사용하여 추가 인수를 받을 수 있습니다.\n",
      "class VAE(keras.models.Model):\n",
      "    def __init__(self, encoder, decoder, **kwargs):\n",
      "        super(VAE, self).__init__(**kwargs)\n",
      "        self.encoder = encoder\n",
      "        self.decoder = decoder\n",
      "\n",
      "        #loss 값을 모니터링 하는 요소\n",
      "        self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
      "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
      "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
      "\n",
      "    #metrics 프로퍼티: VAE 모델의 성능을 모니터링하는 메트릭을 정의합니다. 이 모델은 총 손실 (total_loss), 재구성 손실 (reconstruction_loss), 및 KL-다이버전스 손실 (kl_loss)을 모니터링합니다.\n",
      "    @property\n",
      "    def metrics(self):\n",
      "        return [\n",
      "            self.total_loss_tracker,\n",
      "            self.reconstruction_loss_tracker,\n",
      "            self.kl_loss_tracker,\n",
      "        ]\n",
      "\n",
      "    #call 메서드는 모델을 호출할 때 실행되는 메서드입니다. 입력 이미지를 받아서 잠재 변수를 인코더로부터 추출하고, 해당 잠재 변수를 사용하여 이미지를 디코더로부터 재구성합니다.\n",
      "    #input은 입력 이미지 데이터를 나타냅니다.\n",
      "    #z_mean, z_log_var, z는 인코더로부터 얻은 잠재 변수의 평균, 로그 분산 및 샘플링된 잠재 변수를 나타냅니다.\n",
      "    #reconstruction은 디코더로부터 재구성된 이미지를 나타냅니다.\n",
      "    def call(self, input):\n",
      "        z_mean, z_log_var, z = encoder(input)\n",
      "        reconstruction = decoder(z)\n",
      "        return z_mean, z_log_var, reconstruction\n",
      "    \n",
      "    #train_step 메서드는 VAE 모델의 학습 단계를 정의합니다.\n",
      "    #입력으로 데이터 (data)를 받아서 학습을 진행합니다.\n",
      "    #tf.GradientTape() 내에서 재구성 손실과 KL-다이버전스 손실을 계산합니다.\n",
      "    #reconstruction_loss는 재구성 손실을 계산하는 부분입니다. tf.losses.binary_crossentropy를 사용하여 재구성 손실을 계산하며, 이 값을 500으로 스케일링합니다.\n",
      "    #kl_loss는 KL-다이버전스 손실을 계산하는 부분입니다. 이것은 VAE의 정규화 항으로, 잠재 변수의 분포와 정규 분포 간의 차이를 측정합니다.\n",
      "    #total_loss는 재구성 손실과 KL-다이버전스 손실을 더한 총 손실입니다.\n",
      "    #경사 하강법을 사용하여 총 손실을 최소화하도록 모델을 업데이트하고, 메트릭 값을 업데이트합니다.\n",
      "    #최종적으로 업데이트된 메트릭 값을 딕셔너리로 반환합니다.\n",
      "    def train_step(self, data):\n",
      "        with tf.GradientTape() as tape:\n",
      "            z_mean, z_log_var, reconstruction = self(data)\n",
      "            reconstruction_loss = tf.reduce_mean(500 * tf.losses.binary_crossentropy(data, reconstruction, axis=(1,2,3)))\n",
      "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1))\n",
      "            total_loss = reconstruction_loss + kl_loss\n",
      "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
      "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
      "        self.total_loss_tracker.update_state(total_loss)\n",
      "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
      "        self.kl_loss_tracker.update_state(kl_loss)\n",
      "\n",
      "        return {m.name: m.result() for m in self.metrics}5. VAE 모델 학습과 훈련 단계 정의 및 이미지 데이터로 VAE 모델 학습Python#모델 새로 정의할때\n",
      "vae = VAE(encoder, decoder)우선 위와같이 새로 모델을 정의하고 아래와 같이 훈련을 시켜줍니다.Python#훈련시키기 (훈련후에 결과가 맘에들지않으면 다시 훈련을 시켜줍니다.)\n",
      "vae.compile(optimizer=\"adam\")\n",
      "vae.fit(images, epochs=100, batch_size=320)6. 결과 확인을 위한 이미지 선택 및 재구성 및 원본 이미지와 재구성 이미지 시각화이제 훈련된 자료를 가지고 두개의 이미지를 뽑아 원본과 결과(인코딩-잠재공간-디코딩)를 비교하고 그 두 이미지 사이의 잠재공간을 이용해 다시 디코딩을 하여 결과를 비교해 보겠습니다.Python# 이미지 중에서 무작위로 하나 선택\n",
      "random_index1 = random.randint(0, len(images) - 1)\n",
      "selected_image1 = images[random_index1]\n",
      "\n",
      "# 또 다른 이미지를 무작위로 선택\n",
      "random_index2 = random.randint(0, len(images) - 1)\n",
      "selected_image2 = images[random_index2]\n",
      "\n",
      "# 선택한 이미지 표시\n",
      "plt.figure(figsize=(10, 5))  # 그림 크기 조정\n",
      "plt.subplot(1, 2, 1)  # 1x2 그리드의 첫 번째 위치\n",
      "plt.imshow(selected_image1)\n",
      "plt.title(\"Original Image 1\")\n",
      "\n",
      "# 인코더에 넣어서 임베딩 벡터 확인\n",
      "embedded1 = np.array(encoder.predict(np.array([selected_image1])))[2]\n",
      "\n",
      "# 임베딩 벡터를 디코더에 넣어서 결과값 예측\n",
      "predicted_img1 = decoder.predict(embedded1)\n",
      "plt.subplot(1, 2, 2)  # 1x2 그리드의 두 번째 위치\n",
      "plt.imshow(predicted_img1.reshape(resize_width, resize_height, 3))\n",
      "plt.title(\"Reconstructed Image 1\")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# 선택한 다른 이미지 표시\n",
      "plt.figure(figsize=(10, 5))  # 그림 크기 조정\n",
      "plt.subplot(1, 2, 1)  # 1x2 그리드의 첫 번째 위치\n",
      "plt.imshow(selected_image2)\n",
      "plt.title(\"Original Image 2\")\n",
      "\n",
      "# 인코더에 넣어서 임베딩 벡터 확인\n",
      "embedded2 = np.array(encoder.predict(np.array([selected_image2])))[2]\n",
      "\n",
      "# 임베딩 벡터를 디코더에 넣어서 결과값 예측\n",
      "predicted_img2 = decoder.predict(embedded2)\n",
      "plt.subplot(1, 2, 2)  # 1x2 그리드의 두 번째 위치\n",
      "plt.imshow(predicted_img2.reshape(resize_width, resize_height, 3))\n",
      "plt.title(\"Reconstructed Image 2\")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# 두 이미지의 임베딩 벡터의 중간값을 이용해 결과값 예측\n",
      "predicted_img_combined = decoder.predict((embedded1 + embedded2) / 2)\n",
      "plt.figure(figsize=(5, 5))\n",
      "plt.imshow(predicted_img_combined.reshape(resize_width, resize_height, 3))\n",
      "plt.title(\"Reconstructed Combined Image\")\n",
      "\n",
      "plt.show()훈련양이 그리 많지 않아서 원하지 않은 결과가 나오긴 했지만 디코더로 재구성한 결과를 보면 아래와 같이 나왔습니다. 또한 두 임베딩 벡터사이의 벡터를 이용해 디코딩을 해보면 두 결과의 사이값의 결과가 나왔습니다.결론Variational Autoencoder (VAE)에 대한 개요를 제공하고 파이썬 코드로 VAE를 작성하는 방법을 설명하였습니다. VAE는 딥러닝과 생성 모델링 분야에서 사용되며, 주로 데이터의 차원 축소, 생성, 잠재 변수의 학습, 이미지 생성, 자연어 처리 등 다양한 응용 분야에서 활용됩니다.’요약하자면 아래와 같습니다.VAE는 확률적인 생성 모델 중 하나로, 오토인코더의 변형입니다.VAE는 확률적 잠재 공간을 사용하여 더 유연한 데이터 생성 및 잠재 공간 탐색이 가능합니다.VAE의 핵심 요소로는 인코더와 디코더가 있으며, 인코더는 입력 데이터를 잠재 공간으로 인코딩하고, 디코더는 잠재 공간의 표현을 다시 원래 입력 데이터로 디코딩합니다.VAE의 학습 목표는 재구성 손실과 정규화 손실 을 최소화하는 것입니다.재구성 손실은 입력 데이터와 디코더의 출력 간의 차이를 측정하며, 정규화 손실은 잠재 공간의 확률 분포와 정규 분포 간의 유사성을 측정합니다.VAE를 사용하여 데이터 생성 및 잠재 공간 탐색이 가능하며, 잠재 공간에서 유사한 포인트는 원본 데이터 공간에서 유사한 데이터를 생성합니다.파이썬 코드를 사용하여 VAE 모델을 작성하고 이미지 데이터로 학습하였습니다.이제 코드를 사용하여 VAE 모델을 학습하고, 선택한 이미지의 재구성 및 시각화를 확인할 수 있습니다. VAE는 이미지 생성 및 잠재 공간 탐색과 같은 다양한 응용 분야에서 활용할 수 있는 강력한 생성 모델입니다.함께 참고하면 좋은 글GAN (Generative Adversarial Nets): 생성적 적대 신경망과적합(오버피팅): 딥러닝 기초 시리즈 9합성곱 신경망: 딥러닝 기초 시리즈 8배치 정규화: 딥러닝 기초 시리즈 7가중치 초기화 이론: 딥러닝 기초 시리즈 6역전파를 이용한 학습: 딥러닝 기초 시리즈 5경사 하강법: 딥러닝 기초 시리즈 4손실 함수: 딥러닝 기초 시리즈 3활성화 함수: 딥러닝 기초 시리즈 2퍼셉트론: 딥러닝 기초 시리즈 1     Twitter     Facebook     Pinterest     Email Categories 딥러닝 기초GAN (Generative Adversarial Nets): 생성적 적대 신경망ElevenLabs: AI 더빙 사이트 추천!Leave a Comment 응답 취소CommentName Email Website 다음 번 댓글 작성을 위해 이 브라우저에 이름, 이메일, 그리고 웹사이트를 저장합니다.  글 검색하기  AI 작곡 사이트 suno ai리뷰: 음악의 미래를 열다ElevenLabs: AI 더빙 사이트 추천!VAE (Variational autoencoder): 다변수 오토인코더GAN (Generative Adversarial Nets): 생성적 적대 신경망딥러닝 학습을 위한 텐서플로우 라이브러리(Tensorflow)About UsContact UsTerms and Conditions (이용 약관)개인정보처리방침 © 2024 Tommys works • Built with GeneratePress      Close테크 이야기테크 가이드인공지능 이야기딥러닝 기초딥러닝 튜토리얼홈About UsTerms and Conditions (이용 약관)개인정보처리방침Contact Us게시글 검색하기                     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://alltommysworks.com/vae/'}, page_content='VAE (Variational autoencoder): 다변수 오토인코더\\n\\n10월 8, 2023 by Tommy\\n\\nVAE는 “Variational Autoencoder”의 약자로, 딥러닝과 생성 모델링 분야에서 사용되는 확률적인 생성 모델 중 하나입니다.\\n\\nVAE는 주로 데이터의 차원 축소, 생성, 잠재 변수의 학습, 이미지 생성, 자연어 처리 등 다양한 응용 분야에서 사용됩니다.\\n\\n이글에서는 VAE에 대해 간단히 소개하고 파이썬 코드로 작성 해 보겠습니다.\\n\\n목차\\n\\n\"이 포스팅은 쿠팡 파트너스 활동의 일환으로, 이에 따른 일정액의 수수료를 제공받습니다.\"\\n\\n오토인코더 (Autoencoder):\\n\\nVAE (Variational autoencoder)\\n\\nEncoder와 Decoder의 역할:\\n\\n손실 함수 (Loss Function):\\n\\n생성과 잠재 공간 탐색:\\n\\n파이썬 코드 작성:\\n\\n1. 필요한 라이브러리 가져오기 및 GPU 설정\\n\\n2. Celeb A 데이터셋 이미지 불러오기 및 전처리\\n\\n3. VAE 모델을 위한 인코더, 디코더 및 샘플링 레이어 정의\\n\\n4. VAE (Variational Autoencoder) 모델 클래스 정의와 손실 함수 설정\\n\\n5. VAE 모델 학습과 훈련 단계 정의 및 이미지 데이터로 VAE 모델 학습\\n\\n6. 결과 확인을 위한 이미지 선택 및 재구성 및 원본 이미지와 재구성 이미지 시각화\\n\\n결론\\n\\n참고: VAE 논문\\n\\n오토인코더 (Autoencoder):\\n\\n다변수 오토인코더를 살펴보기 전에, 우선 오토인코더에 대해 알아보겠습니다. 오토인코더는 입력 데이터를 효율적으로 압축하고 복원하는 모델입니다. 이는 데이터의 표현을 학습하기 위해 사용됩니다.\\n\\n오토인코더는 일반적으로 인코더(encoder)와 디코더(decoder) 두 부분으로 구성됩니다. 인코더는 입력 데이터를 저차원 잠재 공간(latent space)으로 인코딩하고, 디코더는 이 잠재 공간의 표현을 다시 원래 입력 데이터로 디코딩합니다.\\n\\nVAE (Variational autoencoder)\\n\\nVAE는 오토 인코더의 변형으로, 확률적 잠재 공간을 사용합니다. 이것은 VAE가 더 유연한 데이터 생성 및 잠재 공간 탐색을 가능하게 합니다.\\n\\nVAE는 잠재 공간의 각 포인트에 대한 확률 분포를 모델링 하며, 이 분포는 일반적으로 정규 분포로 가정 됩니다.\\n\\nEncoder와 Decoder의 역할:\\n\\n인코더: 입력 데이터를 잠재 공간에서의 확률 분포 파라미터(평균과 표준 편차)를 출력합니다. 이 확률 분포를 통해 잠재 공간의 포인트를 샘플링 합니다. 즉 z = 평균 + 분산 * 파라미터 재구성 표준 정규 분포에서 나온 한 개의 샘플링 값 즉z=\\\\mu+\\\\sigma*\\\\epsilon 입니다. 기본적인 오토 인코더와 달리 이 부분에서 학습이 가능합니다.\\n\\n디코더: 이렇게 샘플링된 잠재 포인트를 사용하여 원래 입력 데이터를 재구성합니다.\\n\\n손실 함수 (Loss Function):\\n\\nVAE의 핵심은 재구성 손실(reconstruction loss)과 정규화 손실(regularization loss)을 함께 최소화하려는 것입니다.\\n\\n재구성 손실은 입력 데이터와 디코더의 출력 간의 차이를 측정합니다. 상황에 따라 다르겠지만 디코더의 분포가 베르누이 분포일때는 Cross enthropy Error 를 쓰고 가우시안 분포일때는 Mean Squrered Error 를 이용합니다. 이글에서는 예시로 CEE(Cross Entropy Error)를 이용했습니다. \\\\text{CEE} \\\\ = -\\\\sum P(x)\\\\log{Q(x)}\\n\\n정규화 손실은 잠재 공간의 확률 분포와 정규 분포 간의 유사성을 측정합니다.\\n\\nKullback-Leibler divergence는 두 확률 분포 의 차이를 측정하는 것입니다. 학습은 이 두 분포의 차이를 줄여나가는 방식으로 진행이 됩니다. \\\\text{KL}(P|Q) = \\\\sum\\\\limits P(x) \\\\times \\\\log( \\\\frac{P(x)}{Q(x)} ) 이식은 두 확률분포 P 와 Q 사이의 divergence 즉 차이를 구하는 공식입니다.\\n\\n총 손실 (Total Loss): 이제 재구성 손실과 정규화 손실을 더해줍니다. 즉 \\\\text{Total Loss} = CEE(P,Q) + KL(P|Q) = -\\\\sum P(x)\\\\log{Q(x)} + \\\\sum\\\\limits P(x) \\\\times \\\\log\\\\left( \\\\frac{P(x)}{Q(x)}\\\\right)\\n\\n하지만 KL-divergence를 이용시에 발산하여 NAN에러를 발생할 확률이 크기 때문에 재구성 손실과, 정규화 손실값의 중요도를 적당히 조절해야합니다.\\n\\n생성과 잠재 공간 탐색:\\n\\nVAE를 훈련한 후에는 잠재 공간에서 샘플링하여 새로운 데이터를 생성할 수 있습니다. 이것은 생성 모델로 사용됩니다.\\n\\n잠재 공간에서의 유사한 포인트들은 원본 데이터 공간에서 유사한 데이터를 생성합니다. 따라서 잠재 공간에서의 탐색은 데이터 생성 및 변형에 활용될 수 있습니다.\\n\\n파이썬 코드 작성:\\n\\n이제 VAE를 파이썬 코드로 작성 해 보겠습니다.\\n\\n1. 필요한 라이브러리 가져오기 및 GPU 설정\\n\\nPython\\n\\nimport os\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport random\\nimport tensorflow as tf\\n\\n\\n# GPU 디바이스 목록 가져오기\\n\\ngpus = tf.config.experimental.list_physical_devices(\\'GPU\\')\\n\\nif gpus:\\n    print(\"사용 가능한 GPU 디바이스:\")\\n    for gpu in gpus:\\n        print(\"디바이스 이름:\", gpu.name)\\nelse:\\n    print(\"사용 가능한 GPU 디바이스가 없습니다.\")\\n\\n2. Celeb A 데이터셋 이미지 불러오기 및 전처리\\n\\nPython\\n\\n# 이미지 전처리 코드\\nimage_dir = \"C:\\\\\\\\Users\\\\\\\\crazy\\\\\\\\Downloads\\\\\\\\archive\\\\\\\\img_align_celeba\" #이미지 데이터셋 경로입니다.\\nresize_width, resize_height = 128, 128  # 변경할 크기\\n\\n# 이미지 목록 가져오기\\nimage_files = os.listdir(image_dir)\\n\\n# 이미지를 NumPy 배열로 로드하고 크기 변경하기\\nimages = []\\nfor filename in image_files:\\n    img = tf.keras.preprocessing.image.load_img(os.path.join(image_dir, filename), target_size=(resize_width, resize_height))\\n    img = tf.keras.preprocessing.image.img_to_array(img)\\n    img = img / 255.0  # 이미지 정규화\\n    images.append(img)\\nimages = np.array(images)\\n\\n\\n# 이미지 중에서 무작위로 하나 선택\\nrandom_index = random.randint(0, len(images) - 1)\\nselected_image = images[random_index]\\n\\n3. VAE 모델을 위한 인코더, 디코더 및 샘플링 레이어 정의\\n\\nPython\\n\\nkeras = tf.keras\\nK = tf.keras.backend\\n\\n#샘플링 클래스 정의\\n#keras의 layer를 상속받아 만들어진 클래스 입니다.\\n#call 메서드는 z_mean(평균) 과 z_log_var(로그 분산)을 받습니다.\\n#z_mean 과 z_log_var를 사용해 잠재변수(z)를 return 합니다.\\nclass Sampling(keras.layers.Layer):\\n    def call(self, inputs):\\n        z_mean, z_log_var = inputs\\n        batch = tf.shape(z_mean)[0]\\n        dim = tf.shape(z_mean)[1]\\n        epsilon = K.random_normal(shape=(batch, dim))\\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\\n        \\n#인코더 부분\\nencoder_input = keras.layers.Input(shape=(resize_width, resize_height, 3), name=\"encoder_input\")\\nx = keras.layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\\nx = keras.layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\\nx = keras.layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\\nshape_before_flattening = K.int_shape(x)[1:]\\nx = keras.layers.Flatten()(x)\\nz_mean = keras.layers.Dense(200, name=\"z_mean\")(x)\\nz_log_var = keras.layers.Dense(200, name=\"z_log_var\")(x)\\nz = Sampling()([z_mean, z_log_var])\\nencoder = keras.models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\\n\\n#디코더 부분\\ndecoder_input = keras.layers.Input(shape=(200,), name=\"decoder_input\")\\nx = keras.layers.Dense(np.prod(shape_before_flattening))(decoder_input)\\nx = keras.layers.Reshape(shape_before_flattening)(x)\\nx = keras.layers.Conv2DTranspose(128, (3, 3), strides=2, activation = \\'relu\\', padding=\"same\")(x)\\nx = keras.layers.Conv2DTranspose(64, (3, 3), strides=2, activation = \\'relu\\', padding=\"same\")(x)\\nx = keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation = \\'relu\\', padding=\"same\")(x)\\ndecoder_output = keras.layers.Conv2D(3,(3, 3),strides = 1, activation=\"relu\", padding=\"same\", name=\"decoder_output\")(x)\\ndecoder = keras.models.Model(decoder_input, decoder_output)\\n\\n인코더와 디코더 네트워크는 아래와 같이 구성됩니다.\\n\\n인코더 부분\\n\\nTotal params: 13,200,848\\n\\nTrainable params: 13,200,848\\n\\nNon-trainable params: 0\\n\\n디코더 부분\\n\\nTotal params: 6,827,075\\n\\nTrainable params: 6,827,075\\n\\nNon-trainable params: 0\\n\\n4. VAE (Variational Autoencoder) 모델 클래스 정의와 손실 함수 설정\\n\\nPython\\n\\n#VAE 클래스는 Keras의 Model 클래스를 상속받아서 만들어진 클래스입니다. 이 클래스는 VAE 모델을 정의하고 학습하기 위한 기능을 포함합니다.\\n#__init__ 메서드: 클래스의 생성자로, encoder와 decoder 모델을 인수로 받습니다. 이 두 모델은 VAE의 핵심 구성 요소인 인코더와 디코더입니다.\\n#또한 **kwargs를 사용하여 추가 인수를 받을 수 있습니다.\\nclass VAE(keras.models.Model):\\n    def __init__(self, encoder, decoder, **kwargs):\\n        super(VAE, self).__init__(**kwargs)\\n        self.encoder = encoder\\n        self.decoder = decoder\\n\\n        #loss 값을 모니터링 하는 요소\\n        self.total_loss_tracker = keras.metrics.Mean(name=\\'total_loss\\')\\n        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\\n\\n    #metrics 프로퍼티: VAE 모델의 성능을 모니터링하는 메트릭을 정의합니다. 이 모델은 총 손실 (total_loss), 재구성 손실 (reconstruction_loss), 및 KL-다이버전스 손실 (kl_loss)을 모니터링합니다.\\n    @property\\n    def metrics(self):\\n        return [\\n            self.total_loss_tracker,\\n            self.reconstruction_loss_tracker,\\n            self.kl_loss_tracker,\\n        ]\\n\\n    #call 메서드는 모델을 호출할 때 실행되는 메서드입니다. 입력 이미지를 받아서 잠재 변수를 인코더로부터 추출하고, 해당 잠재 변수를 사용하여 이미지를 디코더로부터 재구성합니다.\\n    #input은 입력 이미지 데이터를 나타냅니다.\\n    #z_mean, z_log_var, z는 인코더로부터 얻은 잠재 변수의 평균, 로그 분산 및 샘플링된 잠재 변수를 나타냅니다.\\n    #reconstruction은 디코더로부터 재구성된 이미지를 나타냅니다.\\n    def call(self, input):\\n        z_mean, z_log_var, z = encoder(input)\\n        reconstruction = decoder(z)\\n        return z_mean, z_log_var, reconstruction\\n    \\n    #train_step 메서드는 VAE 모델의 학습 단계를 정의합니다.\\n    #입력으로 데이터 (data)를 받아서 학습을 진행합니다.\\n    #tf.GradientTape() 내에서 재구성 손실과 KL-다이버전스 손실을 계산합니다.\\n    #reconstruction_loss는 재구성 손실을 계산하는 부분입니다. tf.losses.binary_crossentropy를 사용하여 재구성 손실을 계산하며, 이 값을 500으로 스케일링합니다.\\n    #kl_loss는 KL-다이버전스 손실을 계산하는 부분입니다. 이것은 VAE의 정규화 항으로, 잠재 변수의 분포와 정규 분포 간의 차이를 측정합니다.\\n    #total_loss는 재구성 손실과 KL-다이버전스 손실을 더한 총 손실입니다.\\n    #경사 하강법을 사용하여 총 손실을 최소화하도록 모델을 업데이트하고, 메트릭 값을 업데이트합니다.\\n    #최종적으로 업데이트된 메트릭 값을 딕셔너리로 반환합니다.\\n    def train_step(self, data):\\n        with tf.GradientTape() as tape:\\n            z_mean, z_log_var, reconstruction = self(data)\\n            reconstruction_loss = tf.reduce_mean(500 * tf.losses.binary_crossentropy(data, reconstruction, axis=(1,2,3)))\\n            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1))\\n            total_loss = reconstruction_loss + kl_loss\\n        grads = tape.gradient(total_loss, self.trainable_weights)\\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\\n        self.total_loss_tracker.update_state(total_loss)\\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\\n        self.kl_loss_tracker.update_state(kl_loss)\\n\\n        return {m.name: m.result() for m in self.metrics}\\n\\n5. VAE 모델 학습과 훈련 단계 정의 및 이미지 데이터로 VAE 모델 학습\\n\\nPython\\n\\n#모델 새로 정의할때\\nvae = VAE(encoder, decoder)\\n\\n우선 위와같이 새로 모델을 정의하고 아래와 같이 훈련을 시켜줍니다.\\n\\nPython\\n\\n#훈련시키기 (훈련후에 결과가 맘에들지않으면 다시 훈련을 시켜줍니다.)\\nvae.compile(optimizer=\"adam\")\\nvae.fit(images, epochs=100, batch_size=320)\\n\\n6. 결과 확인을 위한 이미지 선택 및 재구성 및 원본 이미지와 재구성 이미지 시각화\\n\\n이제 훈련된 자료를 가지고 두개의 이미지를 뽑아 원본과 결과(인코딩-잠재공간-디코딩)를 비교하고 그 두 이미지 사이의 잠재공간을 이용해 다시 디코딩을 하여 결과를 비교해 보겠습니다.\\n\\nPython\\n\\n# 이미지 중에서 무작위로 하나 선택\\nrandom_index1 = random.randint(0, len(images) - 1)\\nselected_image1 = images[random_index1]\\n\\n# 또 다른 이미지를 무작위로 선택\\nrandom_index2 = random.randint(0, len(images) - 1)\\nselected_image2 = images[random_index2]\\n\\n# 선택한 이미지 표시\\nplt.figure(figsize=(10, 5))  # 그림 크기 조정\\nplt.subplot(1, 2, 1)  # 1x2 그리드의 첫 번째 위치\\nplt.imshow(selected_image1)\\nplt.title(\"Original Image 1\")\\n\\n# 인코더에 넣어서 임베딩 벡터 확인\\nembedded1 = np.array(encoder.predict(np.array([selected_image1])))[2]\\n\\n# 임베딩 벡터를 디코더에 넣어서 결과값 예측\\npredicted_img1 = decoder.predict(embedded1)\\nplt.subplot(1, 2, 2)  # 1x2 그리드의 두 번째 위치\\nplt.imshow(predicted_img1.reshape(resize_width, resize_height, 3))\\nplt.title(\"Reconstructed Image 1\")\\n\\nplt.show()\\n\\n# 선택한 다른 이미지 표시\\nplt.figure(figsize=(10, 5))  # 그림 크기 조정\\nplt.subplot(1, 2, 1)  # 1x2 그리드의 첫 번째 위치\\nplt.imshow(selected_image2)\\nplt.title(\"Original Image 2\")\\n\\n# 인코더에 넣어서 임베딩 벡터 확인\\nembedded2 = np.array(encoder.predict(np.array([selected_image2])))[2]\\n\\n# 임베딩 벡터를 디코더에 넣어서 결과값 예측\\npredicted_img2 = decoder.predict(embedded2)\\nplt.subplot(1, 2, 2)  # 1x2 그리드의 두 번째 위치\\nplt.imshow(predicted_img2.reshape(resize_width, resize_height, 3))\\nplt.title(\"Reconstructed Image 2\")\\n\\nplt.show()\\n\\n# 두 이미지의 임베딩 벡터의 중간값을 이용해 결과값 예측\\npredicted_img_combined = decoder.predict((embedded1 + embedded2) / 2)\\nplt.figure(figsize=(5, 5))\\nplt.imshow(predicted_img_combined.reshape(resize_width, resize_height, 3))\\nplt.title(\"Reconstructed Combined Image\")\\n\\nplt.show()\\n\\n훈련양이 그리 많지 않아서 원하지 않은 결과가 나오긴 했지만 디코더로 재구성한 결과를 보면 아래와 같이 나왔습니다. 또한 두 임베딩 벡터사이의 벡터를 이용해 디코딩을 해보면 두 결과의 사이값의 결과가 나왔습니다.\\n\\n결론\\n\\nVariational Autoencoder (VAE)에 대한 개요를 제공하고 파이썬 코드로 VAE를 작성하는 방법을 설명하였습니다. VAE는 딥러닝과 생성 모델링 분야에서 사용되며, 주로 데이터의 차원 축소, 생성, 잠재 변수의 학습, 이미지 생성, 자연어 처리 등 다양한 응용 분야에서 활용됩니다.’\\n\\n요약하자면 아래와 같습니다.\\n\\nVAE는 확률적인 생성 모델 중 하나로, 오토인코더의 변형입니다.\\n\\nVAE는 확률적 잠재 공간을 사용하여 더 유연한 데이터 생성 및 잠재 공간 탐색이 가능합니다.\\n\\nVAE의 핵심 요소로는 인코더와 디코더가 있으며, 인코더는 입력 데이터를 잠재 공간으로 인코딩하고, 디코더는 잠재 공간의 표현을 다시 원래 입력 데이터로 디코딩합니다.\\n\\nVAE의 학습 목표는 재구성 손실과 정규화 손실 을 최소화하는 것입니다.\\n\\n재구성 손실은 입력 데이터와 디코더의 출력 간의 차이를 측정하며, 정규화 손실은 잠재 공간의 확률 분포와 정규 분포 간의 유사성을 측정합니다.\\n\\nVAE를 사용하여 데이터 생성 및 잠재 공간 탐색이 가능하며, 잠재 공간에서 유사한 포인트는 원본 데이터 공간에서 유사한 데이터를 생성합니다.\\n\\n파이썬 코드를 사용하여 VAE 모델을 작성하고 이미지 데이터로 학습하였습니다.\\n\\n이제 코드를 사용하여 VAE 모델을 학습하고, 선택한 이미지의 재구성 및 시각화를 확인할 수 있습니다. VAE는 이미지 생성 및 잠재 공간 탐색과 같은 다양한 응용 분야에서 활용할 수 있는 강력한 생성 모델입니다.\\n\\n함께 참고하면 좋은 글\\n\\nGAN (Generative Adversarial Nets): 생성적 적대 신경망\\n\\n과적합(오버피팅): 딥러닝 기초 시리즈 9\\n\\n합성곱 신경망: 딥러닝 기초 시리즈 8\\n\\n배치 정규화: 딥러닝 기초 시리즈 7\\n\\n가중치 초기화 이론: 딥러닝 기초 시리즈 6\\n\\n역전파를 이용한 학습: 딥러닝 기초 시리즈 5\\n\\n경사 하강법: 딥러닝 기초 시리즈 4\\n\\n손실 함수: 딥러닝 기초 시리즈 3\\n\\n활성화 함수: 딥러닝 기초 시리즈 2\\n\\n퍼셉트론: 딥러닝 기초 시리즈 1\\n\\nTwitter\\n\\nFacebook\\n\\nEmail\\n\\nCategories 딥러닝 기초\\n\\nLeave a Comment 응답 취소'),\n",
       " Document(metadata={'source': 'https://alltommysworks.com/gan/'}, page_content='GAN (Generative Adversarial Nets): 생성적 적대 신경망\\n\\n12월 4, 20239월 25, 2023 by Tommy\\n\\nGAN은 생성적 적대 신경망(Generative Adversarial Network)의 약어로, 데이터 생성 및 생성 모델링에 사용되는 딥러닝 아키텍처입니다.\\n\\nGAN은 생성자(Generator)와 판별자(Discriminator)라고 불리는 두 개의 신경망으로 구성되며, 이 두 신경망은 서로 경쟁하면서 학습합니다.\\n\\nGAN에 대해 알아보고 파이썬 텐서 플로우로 작성하여 테스트해보겠습니다.\\n\\n목차\\n\\n\"이 포스팅은 쿠팡 파트너스 활동의 일환으로, 이에 따른 일정액의 수수료를 제공받습니다.\"\\n\\nGAN 이란?\\n\\nGAN 이외의 다양한 생성 모델 종류와 비교\\n\\n적대적 네트워크 (Adversarial nets)\\n\\n위의 식의 각 항들에 대한 설명입니다:\\n\\n훈련시 주의사항:\\n\\n알고리즘 과정\\n\\nGlobal Optimality (전역 최적화) of p_{g} = p_{data}\\n\\n파이썬 과 텐서플로우 라이브러리를 이용한 GAN 구현\\n\\n1. 라이브러리 임포트\\n\\n2. 생성자와 판별자 메서드.\\n\\n3. 초기화\\n\\n4. 자료 로드\\n\\n5. 학습 하기\\n\\n6. 결과 확인\\n\\n결론\\n\\n출처: GAN 논문\\n\\nGAN 이란?\\n\\n생성적 적대 신경망(GAN)은 두 개의 주요 구성 요소인 생성자(G)와 판별자(D)로 이루어져 있습니다. 이 두 신경망은 다중 퍼셉트론으로 구현되며, GAN은 주어진 데이터를 학습하고, 그 데이터와 유사한 새로운 데이터를 생성하는 데 사용됩니다.\\n\\nGAN의 핵심 아이디어는 생성자와 판별자 간의 미니맥스 게임입니다. 즉, 생성자는 가짜 데이터를 생성하여 판별자를 속이고, 판별자는 실제 데이터와 가짜 데이터를 구별하려고 노력합니다.\\n\\nGAN의 목표는 생성자가 실제 데이터 분포를 학습하여 실제와 구별하기 어려운 가짜 데이터를 생성하는 것입니다.\\n\\n이미지 생성 외에도 스타일 변환, 데이터 증강, 이미지 Super-Resolution, 음성 합성 및 다른 응용 분야에서 사용이 될 수 있습니다.\\n\\n생성자(G):\\n\\n생성자는 주어진 무작위 노이즈 벡터를 입력으로 받아 실제 데이터와 유사한 데이터를 생성하는 역할을 합니다.\\n\\nG는 학습 과정에서 생성한 데이터를 판별자(D)에게 전달하고, 이 데이터를 실제 데이터처럼 보이도록 만드는 방향으로 학습합니다.\\n\\nG의 목표는 D를 속이는 것으로, 생성된 데이터가 실제 데이터처럼 판별되도록 노력합니다.\\n\\n판별자(D):\\n\\n판별자는 주어진 데이터가 실제 데이터인지 생성자(G)가 생성한 가짜 데이터인지를 판별하는 역할을 합니다.\\n\\nD는 학습 과정에서 실제 데이터와 생성자가 생성한 데이터를 구별할 수 있도록 훈련됩니다.\\n\\nD의 목표는 실제 데이터를 식별하고 가짜 데이터를 식별하는 데 있어서 정확도를 최대화하는 것입니다.\\n\\n학습 과정에서는 다음과 같은 단계를 거칩니다:\\n\\n무작위 노이즈 벡터를 생성자(G)에 입력으로 주고, G는 가짜 데이터를 생성합니다.\\n\\n생성된 가짜 데이터와 실제 데이터를 함께 판별자(D)에 제공합니다.\\n\\n판별자(D)는 이 두 데이터를 기반으로 가짜와 실제 데이터를 판별하고 손실을 계산합니다.\\n\\n생성자(G)는 판별자를 속이도록 손실을 최소화하기 위해 역전파 알고리즘을 사용하여 가중치를 업데이트합니다.\\n\\n판별자(D)도 자신의 손실을 최소화하기 위해 가중치를 업데이트합니다.\\n\\n이런 과정을 반복하면 생성자는 점차적으로 실제 데이터와 유사한 데이터를 생성하도록 학습하고, 판별자는 생성된 가짜 데이터와 실제 데이터를 더욱 정확하게 판별하도록 학습합니다. GAN은 이 두 신경망 간의 경쟁을 통해 데이터 생성과 판별 능력을 향상시키는 강력한 모델 중 하나입니다.\\n\\nGAN 이외의 다양한 생성 모델 종류와 비교\\n\\nRestricted Boltzmann Machines (RBMs):\\n\\nRBMs은 확률적인 생성 모델로, GAN과 유사한 점은 있지만 GAN보다 복잡한 확률 분포 모델을 학습하기 어려울 수 있습니다.\\n\\nGAN은 생성자와 판별자 간의 경쟁으로 학습되는 반면, RBMs는 에너지 기반 확률 모델로 학습됩니다.\\n\\nDeep Boltzmann Machines (DBMs):\\n\\nDBMs은 여러 층의 제한된 볼츠만 머신(RBM)으로 구성된 생성 모델입니다.\\n\\nDBMs는 GAN과 유사한 확률 모델링을 수행하지만, 더 복잡한 모델 구조를 가질 수 있습니다.\\n\\nDeep Belief Networks (DBNs):\\n\\nDBNs은 하이브리드 모델로, 단일 비방향 레이어와 여러 방향 레이어로 구성됩니다.\\n\\nGAN과 비교하면 DBNs는 방향과 비방향 모델의 계산적 어려움을 동시에 가질 수 있습니다.\\n\\nNoise-Contrastive Estimation (NCE):\\n\\nNCE는 생성 모델의 학습을 위한 대안적인 기준 중 하나로, GAN과 유사한 방식으로 생성 모델을 학습합니다.\\n\\nGAN은 경쟁적인 학습을 사용하고 판별자와 생성자를 별도로 훈련하는 반면, NCE는 생성 모델 자체를 사용하여 학습합니다.\\n\\nGenerative Stochastic Network (GSN):\\n\\nGSN은 생성 모델링의 한 방법으로, Markov 체인을 정의하는 매개변수화된 기계를 학습하는 데 사용됩니다.\\n\\nGAN과 비교하면 GSN은 Markov 체인을 필요로하지 않으며, 생성 시에 피드백 루프가 필요하지 않기 때문에 역전파를 더 효과적으로 활용할 수 있습니다.\\n\\nAdversarial Nets (GAN):\\n\\nGAN은 생성자와 판별자 간의 경쟁을 통해 생성 모델을 학습하는 기술로, GSN과 비교하면 Markov 체인을 사용하지 않습니다.\\n\\nGAN은 피드백 루프 없이 생성하는 데 우수하며, 역전파의 성능을 향상시킬 수 있는 piecewise linear units을 활용 할 수 있습니다.\\n\\n적대적 네트워크 (Adversarial nets)\\n\\n적대적 모델링 프레임워크는 모델이 모두 다층 퍼셉트론인 경우 가장 직접적으로 적용할 수 있습니다.\\n\\n데이터 x에 대한 생성자의 분포 p_g(x)를 학습하기 위해 입력 노이즈 변수 p_z(z)에 대한 사전 분포를 정의하고, 데이터 공간으로의 매핑을 G(z; \\\\theta_g)로 나타냅니다. 여기서 G는 매개변수 \\\\theta_g로 표현되는 미분 가능한 함수입니다. 또한 D(x; \\\\theta_d)라는 두 번째 다층 퍼셉트론을 정의하며, D(x)는 x가 데이터에서 왔을 확률을 나타냅니다. 우리는 D를 학습하여 훈련 예제와 G에서 생성된 샘플에 올바른 레이블을 할당할 확률을 최대화합니다. 동시에 G를 학습하여 \\\\log(1 - D(G(z)))를 최소화합니다.\\n\\n다시 말해, D와 G는 다음과 같은 이분법적 게임(minimax game)을 플레이합니다. 이 게임의 가치 함수 V(G, D)는 다음과 같습니다:\\n\\nV(G, D) = \\\\mathbb{E}_{x\\\\sim p_{\\\\text{data}}(x)} [\\\\log D(x)] + \\\\mathbb{E}_{z\\\\sim p_z(z)} [\\\\log(1 - D(G(z)))] 입니다.여기서 \\\\mathbb{E}(x)는 확률 가중 평균(Probability Weighted Average) 입니다.\\n\\nV(G, D) : 이것은 적대적 네트워크(GAN)에서 사용되는 손실 함수입니다. G와 D는 생성자와 판별자 모델을 나타냅니다. 이 손실 함수는 생성자와 판별자 간의 경쟁적인 게임을 나타냅니다.\\n\\n위의 식의 각 항들에 대한 설명입니다:\\n\\n\\\\mathbb{E} {x\\\\sim p_{\\\\text{data}}(x)} [\\\\log D(x)] :\\n\\nx는 실제 데이터(실제 데이터 분포인 p_{data}\\u200b에서 추출된)를 나타냅니다.\\n\\nD(x)는 판별자 모델이 실제 데이터 x를 실제 데이터로 판별하는 확률을 나타냅니다. 즉, 판별자가 x가 실제 데이터로 판별될 확률을 예측한 값입니다.\\n\\n\\\\text{log }D(x)는 이 확률을 로그 스케일로 변환한 것입니다.\\n\\n이 항은 판별자의 목표를 나타냅니다. 즉, 판별자는 실제 데이터를 최대한 실제 데이터로 판별하도록 학습합니다.\\n\\n\\\\mathbb{E}_{z\\\\sim p_z(z)} [\\\\log(1 - D(G(z)))] :\\n\\nz는 잠재 공간에서 무작위로 샘플링된 잡음 벡터를 나타냅니다. p_z\\u200b(z)는 잡음 벡터의 사전 분포입니다.\\n\\nG(z)는 생성자 모델을 통해 잡음 벡터 z에서 생성된 가짜 데이터를 나타냅니다.\\n\\nD(G(z))는 판별자 모델이 가짜 데이터 G(z)를 실제 데이터로 오인할 확률을 나타냅니다. 이는 판별자가 생성된 가짜 데이터를 얼마나 실제 데이터와 유사하다고 판단하는지를 측정합니다.\\n\\n\\\\text{log} (1−D(G(z)))는 판별자가 G(z)가 가짜 데이터일 확률의 로그값을 나타냅니다.\\n\\n이 항은 생성자의 목표를 나타냅니다. 생성자는 판별자를 속여 가짜 데이터를 최대한 실제 데이터처럼 보이게 만들려고 합니다.\\n\\n훈련시 주의사항:\\n\\n판별자(D)를 훈련 중에 완벽하게 최적화하는 것은 계산상 불가능하며, 유한한 데이터셋에서는 과적합 문제가 발생할 수 있습니다. 판별자(D)와 생성자(G)를 번갈아가며 최적화합니다.\\n\\n판별자(D)를 여러 단계(보통 k 단계) 최적화한 다음 생성자(G)를 1 단계 최적화합니다.\\n\\nSML/PCD(Stochastic Maximum Likelihood/Parallel Contrastive Divergence) 훈련에서 한 학습 단계에서 다음 학습 단계로 샘플을 초기화하지 않고 유지하는 것과 유사합니다.\\n\\n생성자(G)가 잘 학습되지 않을 때, \\\\text{log}(1−D(G(z))) 항은 충분한 경사를 제공하지 못할 수 있습니다.이런 경우에는 대신 \\\\text{log}D(G(z)) 를 최대화하여 학습을 진행합니다. 이렇게 하면 초기 학습 단계에서 더 강한 경사를 얻을 수 있습니다.\\n\\n알고리즘 과정\\n\\nGAN의 미니배치 확률적 경사하강법. 하이퍼 파라미터: 적용시킬 판별자의 스텝수, k 여기서는 k = 1, 최소 1부터 시작합니다.\\n\\n학습 반복 횟수만큼 다음을 반복합니다:\\n\\nk 단계에 대해 다음을 수행합니다:\\n\\n판별자의 학습\\n\\nm개의 잡음 샘플 {z^{(1)}, …, z^{(m)}}를 잡음 사전 분포 p_g(z)에서 샘플링합니다.\\n\\nm개의 예제 {x^{(1)}, …, x^{(m)}}를 데이터 생성 분포 p_{data}(x)에서 샘플링합니다.\\n\\n판별자를 업데이트합니다. 이를 위해 판별자의 가중치 θd 를 경사 상승 방향으로 업데이트합니다. 업데이트는 다음 손실을 최대화하도록 이루어집니다: \\\\nabla{\\\\theta_{d}} \\\\frac{1}{m} \\\\sum_{i=1}^{m} \\\\left[ \\\\log D(x^{(i)}) + \\\\log \\\\left(1 - D(G(z^{(i)}))\\\\right) \\\\right]\\n\\n생성자의 학습\\n\\nm개의 잡음 샘플 {z^(1), …, z^(m)}를 잡음 사전 분포 pg(z)에서 샘플링합니다.\\n\\n생성자를 업데이트합니다. 이를 위해 생성자의 가중치 \\\\theta_g 를 경사 하강 방향으로 업데이트합니다. 업데이트는 다음 손실을 최소화하도록 이루어집니다: \\\\nabla_{\\\\theta_{g}} \\\\frac{1}{m} \\\\sum_{i=1}^{m} \\\\log \\\\left(1 - D(G(z^{(i)}))\\\\right)\\n\\n기울기 기반 업데이트는 표준 기울기 기반 학습 규칙을 사용할 수 있습니다. 실험에서는 모멘텀(momentum)을 사용했습니다.\\n\\n모멘텀 벡터 계산하기: v_{t}=\\\\beta v_{t-1} + (1-\\\\beta) \\\\nabla J(\\\\theta_t) 여기서 v_{t}는 모멘텀 벡터 \\\\beta 는하이퍼 파라미터 (보통 0.9로 설정) \\\\nabla J(\\\\theta_t)는 현재 단계에서의 그래디언트 (미분) 입니다.\\n\\n모멘텀 벡터를 이용하여 파라미터 업데이트: \\\\theta_{t+1}=\\\\theta_{t} - \\\\text{Learning Rate} \\\\times v_t 위의 수식은 GAN의 학습 알고리즘을 설명합니다. 판별자와 생성자가 번갈아가며 업데이트 되며, 이러한 업데이트는 판별자가 실제 데이터와 가짜 데이터를 올바르게 분류하도록 하고, 생성자가 판별자를 속이도록 하는 균형을 찾는 데 도움이 됩니다.\\n\\nGlobal Optimality (전역 최적화) of p_{g} = p_{data}\\n\\n(주요한 식들만 나열해 보겠습니다.)\\n\\nG가 고정된 상황에서 최적의 판별자 D: 이 식은 생성자 G가 고정된 상황에서, 최적의 판별자 D를 나타냅니다. 이 판별자는 데이터 분포 p_{\\\\text{data}}와 생성된 데이터 분포 p_g를 고려하여 주어진 입력 데이터 x가 진짜 데이터인지 (1을 출력) 아니면 생성된 데이터인지 (0을 출력)를 결정하는 함수입니다. 최적의 판별자는 주어진 데이터에 대해 진짜 데이터와 생성된 데이터 간의 확률 비율을 고려하여 판단합니다. D^*G(x) = \\\\frac{p{\\\\text{data}}(x)}{p_{\\\\text{data}}(x) + p_g(x)}\\n\\n생성자와 판별자 간의 훈련 목적 함수 C(G): 이 함수는 생성자 G와 판별자 D 간의 훈련을 지도하기 위한 목적 함수로 사용됩니다. 이 함수는 두 부분으로 나뉘어 있으며, 첫 번째 부분은 실제 데이터 분포 p_{\\\\text{data}}에서 샘플링된 데이터에 대한 기댓값을 계산하고 두 번째 부분은 생성된 데이터 분포 p_g에서 샘플링된 데이터에 대한 기댓값을 계산합니다. 이 함수를 최대화하는 것은 생성자 G가 실제 데이터 분포와 유사한 데이터를 생성하도록 하는 것을 목표로 합니다. C(G) = \\\\mathbb{E}_{x \\\\sim p_{\\\\text{data}}} \\\\left[\\\\log \\\\frac{p_{\\\\text{data}}(x)}{p_{\\\\text{data}}(x) + p_g(x)}\\\\right] + \\\\mathbb{E}_{x \\\\sim p_g} \\\\left[\\\\log \\\\frac{p_g(x)}{p_{\\\\text{data}}(x) + p_g(x)}\\\\right]\\n\\nC(G) 와 Jensen-Shannon 발산 \\\\text{JSD}(p_{\\\\text{data}} | p_g) 간의 관계: 이 식은 목적 함수 C(G)와 Jensen-Shannon 발산 (Jensen-Shannon Divergence, JSD) 사이의 관계를 나타냅니다. C(G) = -\\\\log(4) + 2 \\\\cdot \\\\text{JSD}(p_{\\\\text{data}} | p_g) Jensen-Shannon 발산은 두 확률 분포 간의 유사도를 측정하는 지표로, 두 분포가 유사할수록 값이 작아집니다. 이 관계식은 목적 함수 C(G)를 Jensen-Shannon 발산과 상수 -\\\\log(4)의 합으로 나타냅니다. Jensen-Shannon 발산 (Jensen-Shannon Divergence): Jensen-Shannon 발산은 두 확률 분포 P(x)와 Q(x) 사이의 발산을 측정하는 방법 중 하나로, 두 분포의 평균 분포와의 발산을 나타냅니다. Jensen-Shannon 발산은 다음과 같이 정의됩니다: JS(P∥Q)=\\\\frac{1}{2} D(P∥M)+\\\\frac{1}{2}\\u200bD(Q∥M) 및 의 식의 M은 P와 Q의 평균 분포를 나타냅니다: M(x)=\\\\frac{1}{2} \\u200b(P(x)+Q(x)) Jensen-Shannon 발산은 항상 0 이상의 값을 가지며, 두 분포가 정확히 같을 때에만 0 이 됩니다. Jensen-Shannon 발산은 발산의 대칭성을 가지며, 두 분포가 얼마나 유사한지를 측정하는 데 사용됩니다. 따라서 GAN(Generative Adversarial Network)에서 생성자와 실제 데이터 분포 간의 유사성을 측정하는 데 많이 활용됩니다.\\n\\nC(G)의 전역 최소값: 이 식은 목적 함수 C(G)가 최소값을 가질 때의 값을 나타냅니다. C^*는 C(G)의 최소값을 나타내며, 이 값은 p_g가 p_{\\\\text{data}}와 거의 일치할 때 달성됩니다. 이것은 생성자가 실제 데이터 분포를 거의 완벽하게 모방했을 때의 상황을 나타내며, C^*는 목적 함수가 최소가 되는 값으로 정의됩니다. 즉, 생성된 데이터 분포 p_g가 실제 데이터 분포 p_{\\\\text{data}}와 구별할 수 없을 때 목적 함수가 최소값이 됩니다. 이 최소값을 구체적으로 계산하면 C^* = -\\\\log(4)가 됩니다. 이 값일 때, 생성된 분포와 실제 분포가 완전히 일치하고 목적 함수 C(G)는 최소가 됩니다.\\n\\n파이썬 과 텐서플로우 라이브러리를 이용한 GAN 구현\\n\\nGAN 논문을 바탕으로 파이썬과 텐서 플로우로 구현을 시작해 보겠습니다. 이 예제에서는 기본적인 Mnist 데이터 셋을 이용하였습니다.\\n\\n1. 라이브러리 임포트\\n\\n우선 기본적으로 필요한 라이브러리를 불러옵니다.\\n\\nPython\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport tensorflow as tf\\n\\n2. 생성자와 판별자 메서드.\\n\\n텐서플로우의 케라스를 기준으로 생성자와 판별자의 모델을 정의 했습니다.\\n\\nPython\\n\\n# 생성자 구현\\ndef build_generator(input_dim, output_dim, hidden_dim):\\n    model = tf.keras.Sequential()\\n    model.add(tf.keras.layers.Dense(hidden_dim, input_dim=input_dim, activation=\\'relu\\'))\\n    model.add(tf.keras.layers.Dense(output_dim, activation=\\'sigmoid\\'))\\n    return model\\n\\n# 판별자 구현\\ndef build_discriminator(input_dim, hidden_dim):\\n    model = tf.keras.Sequential()\\n    model.add(tf.keras.layers.Dense(hidden_dim, input_dim=input_dim, activation=\\'relu\\'))\\n    model.add(tf.keras.layers.Dense(1, activation=\\'sigmoid\\'))\\n    return model\\n\\n3. 초기화\\n\\n초기에 필요한 값들을 초기화 해 줍니다.\\n\\nPython\\n\\nrandom_dim = 100  # 잡음 벡터의 차원\\ndata_dim = 784  # MNIST 데이터의 차원\\nhidden_dim = 128  # 은닉층의 차원\\nbatch_size = 64  # 미니배치 크기\\nepochs = 20000  # 학습 에포크 수\\n\\n# 생성자와 판별자 모델 생성\\ngenerator = build_generator(random_dim, data_dim, hidden_dim)\\ndiscriminator = build_discriminator(data_dim, hidden_dim)\\n\\n# 판별자 컴파일\\ndiscriminator_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)  # 모멘텀 옵티마이저 설정\\ndiscriminator.compile(loss=\\'binary_crossentropy\\', optimizer=discriminator_optimizer)\\n\\n# GAN 모델 생성\\ndiscriminator.trainable = False  # 판별자는 학습하지 않도록 설정\\ngan_input = tf.keras.Input(shape=(random_dim,))\\nx = generator(gan_input)\\ngan_output = discriminator(x)\\ngan = tf.keras.Model(gan_input, gan_output)\\n\\ngan_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)  # 모멘텀 옵티마이저 설정\\ngan.compile(loss=\\'binary_crossentropy\\', optimizer=gan_optimizer)\\n\\n4. 자료 로드\\n\\n학습할 자료를 로드합니다. 여기서는 기본으로 Mnist데이터를 이용했습니다.\\n\\nPython\\n\\n# MNIST 데이터 로드\\n(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\\nX_train = X_train / 255.0  # 0~1 범위로 정규화\\nX_train = X_train.reshape(X_train.shape[0], data_dim)\\n\\n5. 학습 하기\\n\\n이제 학습을 시작하면 됩니다.\\n\\nPython\\n\\n# GAN 학습\\nfor epoch in range(epochs):\\n    # 판별자 학습\\n    noise = np.random.normal(0, 1, size=[batch_size, random_dim])  # 잡음 생성\\n    generated_data = generator.predict(noise)  # 생성자로 가짜 데이터 생성\\n\\n    real_data = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]  # 실제 데이터 샘플링\\n\\n    X = np.concatenate([real_data, generated_data])  # 실제 데이터와 가짜 데이터 병합\\n    y_dis = np.zeros(2 * batch_size)\\n    y_dis[:batch_size] = 1  # 실제 데이터는 1로 레이블\\n\\n    discriminator.trainable = True\\n    d_loss = discriminator.train_on_batch(X, y_dis)  # 판별자 학습\\n\\n    # 생성자 학습\\n    noise = np.random.normal(0, 1, size=[batch_size, random_dim])\\n    y_gen = np.ones(batch_size)  # 생성자는 가짜 데이터를 생성하므로 레이블은 1\\n\\n    discriminator.trainable = False\\n    g_loss = gan.train_on_batch(noise, y_gen)  # 생성자 학습\\n\\n    # 매 1000 에포크마다 결과 출력\\n    if epoch % 100 == 0:\\n        print(f\"Epoch {epoch}, D Loss: {d_loss}, G Loss: {g_loss}\")\\n\\n        # 생성된 이미지를 저장\\n        # 생성된 이미지를 저장\\n        generated_images = generator.predict(noise)\\n\\n        # 이미지를 4x4 배열로 표시\\n        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\\n        for i in range(4):\\n            for j in range(4):\\n                ax = axes[i, j]\\n                ax.imshow(generated_images[i * 4 + j].reshape(28, 28), interpolation=\\'nearest\\', cmap=\\'gray_r\\')\\n                ax.axis(\\'off\\')\\n\\n        plt.subplots_adjust(wspace=0.1, hspace=0.1)\\n        plt.savefig(f\"gan_generated_image_epoch_{epoch}.png\")\\n        plt.close()\\n\\n6. 결과 확인\\n\\n이제 학습한 결과를 보면 아래와 같은 결과가 나오게 됩니다.\\n\\n결론\\n\\n“이 포스팅은 쿠팡 파트너스 활동의 일환으로, 이에 따른 일정액의 수수료를 제공받습니다.”\\n\\nGAN (Generative Adversarial Network)은 생성자와 판별자로 이루어진 딥러닝 아키텍처로, 데이터 생성과 생성 모델링에 사용됩니다. 생성자는 가짜 데이터를 생성하고, 판별자는 실제 데이터와 가짜 데이터를 구별하려고 학습합니다. 이를 통해 생성자는 실제 데이터와 유사한 가짜 데이터를 생성하도록 학습하고, 판별자는 더욱 정확하게 가짜 데이터와 실제 데이터를 판별하도록 학습합니다. GAN은 이미지 생성부터 스타일 변환, 데이터 증강, 이미지 Super-Resolution, 음성 합성 등 다양한 응용 분야에서 사용됩니다.\\n\\n또한 GAN의 훈련 과정은 어려움을 겪을 수 있으며, 모드 붕괴와 같은 문제가 발생할 수 있습니다. 이러한 문제에 대한 해결 방법과 최신 GAN 변형 모델에 대한 연구 동향이 중요합니다. 또한 GAN과 같은 생성 모델의 윤리적 고려 사항에 대한 인식도 필요합니다. 개인 정보 보호와 딥페이크와 같은 기술을 통한 위조에 대한 주의가 필요합니다.\\n\\n또한 파이썬과 텐서플로우로 구현을 해보았습니다. GAN은 생성 모델링 분야에서 혁신적인 모델 중 하나로, 더욱 발전해나가며 다양한 응용 분야에 적용될 것으로 기대됩니다.\\n\\n함께 참고하면 좋은 글\\n\\n딥러닝 학습을 위한 텐서플로우 라이브러리(Tensorflow)\\n\\n인공지능 을 위한 아나콘다 와 VS code 설치법\\n\\nTwitter\\n\\nFacebook\\n\\nEmail\\n\\nCategories 딥러닝 기초\\n\\n1 thought on “GAN (Generative Adversarial Nets): 생성적 적대 신경망”\\n\\nPingback: VAE (Variational autoencoder): 다변수 오토인코더\\n\\nComments are closed.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://alltommysworks.com/vae/\",\n",
    "    \"https://alltommysworks.com/gan/\"\n",
    "]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Documents Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"docs/doc_1.pdf\")\n",
    "pages = loader.load_and_split() # 페이지별 스플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,3,2,1)\n",
      "NoonesleepinTokyoAllrightcrossingthelineNoonequittheradioTokyoisonfire\n",
      "Evenifyousay:I havebeentheworldwideI'lltakeyouwheresurelyyouhaveneverbeenAllnightinthefightI'mokay, comeonComeon\n",
      "HeydoyoufeelthenightisbreathableLookatthistownwhichisunbelievableNootherplaceslikethatintheworld,world,world\n",
      "(1,2,3,4)\n",
      "NoonesleepinTokyoAllnightcrossingthelineNoonequittheradioTokyoisonfire\n",
      "NoonesleepinTokyoAllnightcrossingthelineNoonequittheradioTokyoisonfire\n",
      "TurningtothelefteasychicksandredlightsAndtotherightcrazymusiceverywhereAllrightinthefightI'mokay, comeonComeon\n",
      "HeydoyoufeelthenightisbreathableLookatthistownwhichisunbelievableNootherplaceslikethatintheworld,world,world\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"docs/doc_1.docx\")\n",
    "data =loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 2, 1)\n",
      "\n",
      "No one sleep in Tokyo\n",
      "All right crossing the line\n",
      "No one quit the radio\n",
      "Tokyo is on fire\n",
      "\n",
      "Even if you say: I have been the world wide\n",
      "I'll take you where surely you have never been\n",
      "All night in the fight I'm okay, come on\n",
      "Come on\n",
      "\n",
      "Hey do you feel the night is breathable\n",
      "Look at this town which is unbelievable\n",
      "No other places like that in the world, world, world\n",
      "\n",
      "(1, 2, 3, 4)\n",
      "\n",
      "No one sleep in Tokyo\n",
      "All night crossing the line\n",
      "No one quit the radio\n",
      "Tokyo is on fire\n",
      "\n",
      "No one sleep in Tokyo\n",
      "All night crossing the line\n",
      "No one quit the radio\n",
      "Tokyo is on fire\n",
      "\n",
      "Turning to the left easy chicks and red lights\n",
      "And to the right crazy music everywhere\n",
      "All right in the fight I'm okay, come on\n",
      "Come on\n",
      "\n",
      "Hey do you feel the night is breathable\n",
      "Look at this town which is unbelievable\n",
      "No other places like that in the world, world, world\n",
      "\n",
      "(1, 2, 3, 4)\n",
      "\n",
      "No one sleep in Tokyo\n",
      "All night crossing the line\n",
      "No one quit the radio\n",
      "Tokyo is on fire\n",
      "\n",
      "No one sleep in Tokyo\n",
      "All night crossing the line\n",
      "No one quit the radio\n",
      "Tokyo is on fire\n",
      "\n",
      "(Come on)\n",
      "\n",
      "(1, 2, 3, 4)\n",
      "\n",
      "All night crossing the line\n",
      "Tokyo is on fire\n",
      "\n",
      "Hey do you feel the night is breathable\n",
      "Look at this town which is unbelievable\n",
      "No other places like that in the world, world, world\n",
      "\n",
      "(1, 2, 3, 4)\n",
      "\n",
      "No one sleep in Tokyo\n",
      "All night crossing the line\n",
      "No one quit the radio\n",
      "Tokyo is on fire\n",
      "\n",
      "No one sleep in Tokyo\n",
      "All night crossing the line\n",
      "No one quit the radio\n",
      "Tokyo is on fire\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path = \"csv/snp500_history.csv\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'csv/snp500_history.csv', 'row': 0}, page_content='Date: 1927-12-30\\nOpen: 17.660000\\nHigh: 17.660000\\nLow: 17.660000\\nClose: 17.660000\\nAdj Close: 17.660000\\nVolume: 0'),\n",
       " Document(metadata={'source': 'csv/snp500_history.csv', 'row': 1}, page_content='Date: 1928-01-03\\nOpen: 17.760000\\nHigh: 17.760000\\nLow: 17.760000\\nClose: 17.760000\\nAdj Close: 17.760000\\nVolume: 0')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
